mode: train
gpu_ids: [0]
scale: 4 #todo
run_range: 1
# mask_training: null

datasets:
    train_CAVE: #todo train datasets
        setmode: LRHRRAM
        data_type: .npy
        data_root: ../dataset/CAVE/test/
        scaledict:
            LR: 1
            REF: 4
            GT: 4
        img_range: 65536
        LRdim: 31
        REFdim: 3
        n_workers: 4
        repeat: 3
        batch_size: 8   #todo:
        patch_size: 18
        use_flip: true
        use_rot: true
        noise: .

    valid_CAVE: #todo validation datasets
        setmode   : LRHRRAM
        data_type : .npy
        data_root : ../dataset/CAVE/test/
        scaledict : 
            LR        : 1
            REF       : 4
            GT        : 4
        img_range : 65536
        LRdim     : 31
        REFdim    : 3
        batch_size: 1
        n_workers : 4
        noise     : .
## hyper-parameters for network architecture
networks:
    net_arch: sst # this v alue must be same with the filename of 'your_network_name'.py
    numLayers: 2
    convDim: 120
    numHeads: 8
    patchSize: 1
    poolSize: 4
    ksize: 9
    # learning_rate: 0.0002

# the setting for optimizer, loss function, learning_strategy, etc.
solver:
    loss_name: l1
    optimType: ADAM
    learning_rate: 0.0002
    lr_scheme: warm_up # warm_up or multisteplr
    warmUpEpoch: 0 # for warm_up
    lrStepSize: 200 # for multisteplr
    weight_decay: null #todo: 0.0001
    acuSteps: 1 #todo:
    manual_seed: 0
    num_epochs: 1000
    save_ckp_step: 1000
    pretrain: null
    pretrained_path: experiments/FinalAblation/3_base_conv_patchv2_transformer_B8P18_lr0002_warm_up/epochs/last_ckp.pth


logger:
    name: 
    tags: [HyperParam, CAVE] #ablation, Hyper
    tag: null
    log_dir: Ablation_study/